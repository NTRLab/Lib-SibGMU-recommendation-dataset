{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH1Z8sJB6Mpe",
    "outputId": "833d0a0e-e449-4c71-ed22-22f957d9baa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: lightfm in /usr/local/lib/python3.7/dist-packages (1.16)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2022.5.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (3.1.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: implicit in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from implicit) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from implicit) (4.64.0)\n",
      "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from implicit) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightfm\n",
    "!pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "xgPYsuDKVXIj"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import implicit\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightfm import LightFM\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtUKg1-jVXIr"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "pxX33pD4VXIr"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('medlib_dataset.csv')\n",
    "name_books = pd.read_csv('books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "VMVRgKxPVXIs"
   },
   "outputs": [],
   "source": [
    "name_books.fillna('', inplace=True)\n",
    "name_books = name_books[name_books.id_book != '']\n",
    "name_books = name_books[name_books.name_book != '']\n",
    "name_books.name_book = name_books.name_book.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAr9BZEzVXIt"
   },
   "source": [
    "## Helper functions for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "73rlN44gVXIs"
   },
   "outputs": [],
   "source": [
    "# Helper function for all\n",
    "def add_categories(df):\n",
    "    item_user = df[['id', 'id_book']]\n",
    "    \n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        item_user.id = item_user.id.astype('category')\n",
    "        item_user.id_book = item_user.id_book.astype('category')\n",
    "    \n",
    "        item_user['category_id'] = item_user.id.cat.codes\n",
    "        item_user['category_book'] = item_user.id_book.cat.codes\n",
    "    \n",
    "    return item_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "7PoAVLjVVXIt"
   },
   "outputs": [],
   "source": [
    "def create_ds_for_clustering(df, semestr):\n",
    "    '''\n",
    "    One-Hot Encoding vector for each user\n",
    "    '''\n",
    "    df_clustering = df[df.semestr.isin(semestr)][['category_id', \n",
    "                                                  'category_book']]\n",
    "    df_clustering = df_clustering.drop_duplicates()    \n",
    "    # add ohe vector for user\n",
    "    X_train = pd.merge(\n",
    "        df_clustering, pd.get_dummies(df_clustering['category_book'], \n",
    "                                      prefix='book'),\n",
    "        left_index=True, right_index=True, \n",
    "        how='inner').drop_duplicates().drop('category_book', axis=1).groupby('category_id').sum().sort_index()\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "CfTomhbFVXIt"
   },
   "outputs": [],
   "source": [
    "def find_best_k(X_train):\n",
    "    '''\n",
    "    Training K-Means with several clusters\n",
    "    Returns best silhouette coefficient\n",
    "    '''\n",
    "    metrics = {}\n",
    "    for i in range(3, 8):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "        kmeans.fit(X_train)\n",
    "\n",
    "        cluster_labels = kmeans.fit_predict(X_train)\n",
    "        silhouette_avg = silhouette_score(X_train, cluster_labels)\n",
    "        \n",
    "        metrics[i] = silhouette_avg\n",
    "\n",
    "    best_k = max(metrics, key=metrics.get)\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "heV97o9qVXIu"
   },
   "outputs": [],
   "source": [
    "def fit_kmeans(X_train, clusters):\n",
    "    '''\n",
    "    K-Means training\n",
    "    '''\n",
    "    kmeans = KMeans(n_clusters=clusters,\n",
    "                    init='k-means++',\n",
    "                    random_state=42)\n",
    "    \n",
    "    y_km = kmeans.fit(X_train)\n",
    "    return y_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "jFZDRgozVXIu"
   },
   "outputs": [],
   "source": [
    "def create_kmean_df(y_km, X_train):\n",
    "    '''\n",
    "    Creates dataframe that looks like this: \n",
    "        |_category_id_|_cluster_|\n",
    "        |     102     |    0    |\n",
    "    '''\n",
    "    clusters = y_km.predict(X_train)\n",
    "\n",
    "    labels = pd.DataFrame(clusters)\n",
    "    users = pd.DataFrame(X_train.index.unique())\n",
    "\n",
    "    kmeans_df = pd.concat((users, labels), axis=1)\n",
    "    kmeans_df.rename({0: 'cluster'}, axis=1, inplace=True)\n",
    "    \n",
    "    return kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "BeabNDlrVXIu"
   },
   "outputs": [],
   "source": [
    "def hit_rate_kmean(kmeans_df, df, semestr, topn:int=10):\n",
    "    '''\n",
    "    Hit Rate for 1 specific book\n",
    "    '''\n",
    "    users = kmeans_df.category_id.unique()\n",
    "    score = 0\n",
    "\n",
    "    for user in users:\n",
    "        cluster = kmeans_df[kmeans_df.category_id == user]['cluster'].unique()[0]\n",
    "        similar_users = kmeans_df[kmeans_df.cluster == cluster]['category_id'].unique()\n",
    "\n",
    "        rec_books = df[(df.category_id.isin(similar_users)) & (df.semestr == semestr) & (\n",
    "            df.category_id != user)]['id_book'].value_counts()[:topn].index.tolist()\n",
    "\n",
    "        true_book = df[(df.category_id == user) & (df.semestr == semestr)].sort_values(by='date')['id_book'].values[0]\n",
    "        if true_book in rec_books:\n",
    "            score += 1\n",
    "\n",
    "    hit_rate = score / users.shape[0]\n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "x7MJkXf-VXIu"
   },
   "outputs": [],
   "source": [
    "def test_clustering(df, train_sem: list, test_sem: int):    \n",
    "    # Clustering data\n",
    "    X_train = create_ds_for_clustering(df, train_sem)\n",
    "    \n",
    "    # Optimal number of clusters\n",
    "    clusters = find_best_k(X_train)\n",
    "    \n",
    "    # Train kmeans with best k\n",
    "    y_km = fit_kmeans(X_train, clusters)\n",
    "    \n",
    "    # Users from n semester\n",
    "    users_n_semester = df[df.semestr == test_sem]['category_id'].unique()\n",
    "    \n",
    "    # One-Hot Encoding for users from n semester\n",
    "    X_test = X_train[X_train.index.isin(users_n_semester)]\n",
    "    \n",
    "    # Df with users and their clusters\n",
    "    kmean_df = create_kmean_df(y_km, X_test)\n",
    "    dct = {}\n",
    "    dct['semestr'] = test_sem\n",
    "    print(f'\\t\\tCluster model Metrics for {test_sem} semester with {clusters} clusters')\n",
    "\n",
    "    for n in [1, 5, 10]:\n",
    "        hr = 100 * round(hit_rate_kmean(kmean_df, df, test_sem, topn=n), 2)\n",
    "        dct[f'HitRate@{n}'] = hr\n",
    "        print(f'\\tHit Rate@{n}:\\t\\t{hr}%')\n",
    "\n",
    "    print('\\n\\n')\n",
    "    dct['clusters'] = clusters\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9TpP3u0VXIv"
   },
   "source": [
    "## Helper function for ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "A5gMjNscVXIv"
   },
   "outputs": [],
   "source": [
    "def csr_row_set_nz_to_val(csr, row, value=0):\n",
    "    if not isinstance(csr, csr_matrix):\n",
    "        raise ValueError('Matrix given must be of CSR format.')\n",
    "        \n",
    "    csr.data[csr.indptr[row]:csr.indptr[row+1]] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "AE7gLIw0VXIv"
   },
   "outputs": [],
   "source": [
    "def create_csr_matrix(matrix, users, items):\n",
    "    '''\n",
    "    Creating CSR Matrix\n",
    "    '''\n",
    "    n_users = users.max() + 1\n",
    "    n_items = items.max() + 1\n",
    "\n",
    "    matrix_shape = (n_users, n_items)\n",
    "    data = np.ones(items.shape)\n",
    "    \n",
    "    user_item_matrix = csr_matrix((data, (users, items)), shape=matrix_shape)\n",
    "    return user_item_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "yEj81Mj4VXIv"
   },
   "outputs": [],
   "source": [
    "def normalization_matrix(user_item_matrix):\n",
    "    '''\n",
    "    Normalize matrix for user\n",
    "    '''\n",
    "    reader_normalized_matrix = normalize(user_item_matrix, axis=1, norm='l1')\n",
    "    book_normalized_matrix = normalize(user_item_matrix, axis=0, norm='l1')\n",
    "\n",
    "    lazy_readers = np.array(user_item_matrix.sum(axis=1)).squeeze()\n",
    "    lazy_readers = np.where(lazy_readers < 2)[0]    \n",
    "\n",
    "    for rId in lazy_readers:\n",
    "        csr_row_set_nz_to_val(reader_normalized_matrix, rId)\n",
    "\n",
    "    return reader_normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "vtLqM4LJVXIv"
   },
   "outputs": [],
   "source": [
    "def normalize_matrix(train, val=None):\n",
    "    '''\n",
    "    Split matrix to train/val\n",
    "    '''\n",
    "    users = train.category_id\n",
    "    items = train.category_book\n",
    "\n",
    "    csr_train = create_csr_matrix(train, users, items)\n",
    "    csr_val = create_csr_matrix(val, users, items)\n",
    "\n",
    "    normalization_train = normalization_matrix(csr_train)\n",
    "    normalization_val = normalization_matrix(csr_val)\n",
    "    \n",
    "    return normalization_train, normalization_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "dGRIAdLtVXIv"
   },
   "outputs": [],
   "source": [
    "def hit_rate_als(model, df, train, test, normalization_train, items_max, n, test_sem=0, verbose=True):\n",
    "    '''\n",
    "    Hit Rate for 1 specific book\n",
    "    '''\n",
    "    users = test.category_id.unique()\n",
    "    score = 0\n",
    "    \n",
    "    for user in users: \n",
    "      indexs = model.recommend(int(user), normalization_train, filter_already_liked_items=False)[0][:n].tolist()\n",
    "      if test_sem != 0:\n",
    "        actual = test[(test.category_id == user) & (test.semestr == test_sem)]['id_book'].values.tolist()\n",
    "      else:\n",
    "        actual = test[test.category_id == user]['id_book'].values.tolist()\n",
    "\n",
    "      preds = df[df.category_book.isin(indexs)]['id_book'].unique().tolist()\n",
    "\n",
    "      hit = len(set(actual) & set(preds))\n",
    "      if hit > 0:\n",
    "        score += 1\n",
    "      hite_rate = score / users.shape[0]\n",
    "      \n",
    "    return hite_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "VrLos-irVXIv"
   },
   "outputs": [],
   "source": [
    "def train_validation(train):\n",
    "    val_shape = int(train.id.unique().shape[0] * (15 / 100))\n",
    "    not_choose = train.groupby(\n",
    "        'id')['id_book'].count()[train.groupby(\n",
    "            'id')['id_book'].count() == 1].index.tolist()\n",
    "\n",
    "    val_users = np.random.choice(train[~train.id.isin(not_choose)]['id'].unique(), val_shape)\n",
    "\n",
    "    val = train[train.id.isin(val_users)].sort_values(by=['id', 'date']).groupby('id').last().reset_index()\n",
    "    new_train = train[~train.apply(tuple,1).isin(val.apply(tuple,1))]\n",
    "\n",
    "    return new_train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "gP_B6ZMLVXIw"
   },
   "outputs": [],
   "source": [
    "def tuning_als(df, normalization_train, train, X_val, items):\n",
    "    lst = []\n",
    "    for factor in [40]:\n",
    "        for iters in [40]:\n",
    "            for regular in [0.05]:\n",
    "                LogMatFac = implicit.als.AlternatingLeastSquares(\n",
    "                    factors=factor,\n",
    "                    regularization=regular,\n",
    "                    iterations=iters,\n",
    "                    random_state=42,\n",
    "                    num_threads=4\n",
    "                )\n",
    "\n",
    "                LogMatFac.fit(normalization_train, show_progress=False)\n",
    "                scores = []\n",
    "\n",
    "                for n in [1, 5, 10]:\n",
    "                  scores.append(hit_rate_als(LogMatFac,\n",
    "                                              df,\n",
    "                                              train,\n",
    "                                              X_val,\n",
    "                                              normalization_train,\n",
    "                                              items,\n",
    "                                              n=n,\n",
    "                                              verbose=False\n",
    "                                              )\n",
    "                                 )\n",
    "                dct = {}\n",
    "                dct['factors'] = factor\n",
    "                dct['regularization'] = regular\n",
    "                dct['iterations'] = iters\n",
    "                dct['score1'] = scores[0]\n",
    "                dct['score5'] = scores[1]\n",
    "                dct['score10'] = scores[2]\n",
    "                dct['model'] = LogMatFac\n",
    "                lst.append(dct)\n",
    "                \n",
    "    best_params = max(lst, key=lambda x: x['score5'])\n",
    "    \n",
    "    return best_params['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "ey6WdJb5VXIw"
   },
   "outputs": [],
   "source": [
    "def test_als(df, train_sem: list, test_sem: int):    \n",
    "    train = df[df.semestr.isin(train_sem)]\n",
    "    train, validation = train_validation(train)\n",
    "    normalization_train, val_ = normalize_matrix(train=train, val=validation)\n",
    "\n",
    "    X_test = df[df.semestr == test_sem]\n",
    "    items = 1748\n",
    "\n",
    "    model = tuning_als(df, normalization_train, train, validation, items)\n",
    "    \n",
    "    dct = {}\n",
    "    dct['semestr'] = test_sem\n",
    "    \n",
    "    print(f'\\n\\t\\tALS model Metrics for {test_sem} semester\\n') \n",
    "\n",
    "    for n in [1, 5, 10]:\n",
    "        hr = 100 * round(hit_rate_als(model, df, train, X_test, normalization_train, items, n, test_sem), 3)\n",
    "        dct[f'HitRate@{n}'] = hr\n",
    "        print(f'\\tHit Rate@{n}:\\t\\t{hr:.2f}%')\n",
    "\n",
    "    print('\\n\\n')\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FLAZ3MYe9u_"
   },
   "source": [
    "## Helper function for LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Z6cKHDRao1Gh"
   },
   "outputs": [],
   "source": [
    "def hit_rate_lightfm(model, train, test, items_max, test_sem=0, n:int=10, verbose=True):\n",
    "    users = test.category_id.unique()\n",
    "    score = 0\n",
    "    for user in users:\n",
    "      indexs = model.predict(int(user), np.arange(items_max)).argsort()[::-1]\n",
    "      already_read = train[(train.category_id == int(user)) & (\n",
    "          train.category_book.isin(indexs))]['category_book'].tolist()\n",
    "      for ar in already_read:\n",
    "        indexs = indexs[indexs != ar]\n",
    "      indexs = indexs[:n]\n",
    "      \n",
    "      if test_sem != 0:\n",
    "        actual = test[(test.category_id == user) & (test.semestr == test_sem)]['id_book'].values\n",
    "      else:\n",
    "        actual = test[test.category_id == user]['id_book'].values\n",
    "        \n",
    "      preds = train[train.category_book.isin(indexs)]['id_book'].unique().tolist()\n",
    "      \n",
    "      hit = len(set(actual) & set(preds))\n",
    "      if hit > 0:\n",
    "        score += 1\n",
    "    hit_rate = score / users.shape[0]\n",
    "    \n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "OzIGB2BgfEoZ"
   },
   "outputs": [],
   "source": [
    "def sample_hyperparameters():\n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.choice([20, 40, 60], 1)[0],\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"loss\": np.random.choice([\"bpr\", \"warp\", \"warp-kos\", 'logistic']),\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_sampled\": np.random.choice([5, 10, 15], 1)[0],\n",
    "            \"num_epochs\": np.random.choice([20, 40, 60], 1)[0],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "5P9y5W-xfhOI"
   },
   "outputs": [],
   "source": [
    "def random_search(normalization_train, train, val, items_max, num_samples, num_threads=8):\n",
    "  for hyperparams in itertools.islice(sample_hyperparameters(), num_samples):\n",
    "    num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "\n",
    "    model = LightFM(**hyperparams)\n",
    "    model.fit(normalization_train, epochs=num_epochs, num_threads=num_threads)\n",
    "    \n",
    "    scores = []\n",
    "    for n in [1, 5, 10]:\n",
    "      hr = hit_rate_lightfm(model, train, val, items_max, n=n)\n",
    "      scores.append(hr)\n",
    "    \n",
    "    hyperparams[\"num_epochs\"] = num_epochs\n",
    "    \n",
    "    yield (scores, hyperparams, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "mUCBbzKGgJIE"
   },
   "outputs": [],
   "source": [
    "def test_lightfm(df, train_sem: list, test_sem: int):    \n",
    "    train = df[df.semestr.isin(train_sem)]\n",
    "    \n",
    "    train, validation = train_validation(train)\n",
    "\n",
    "    normalization_train, normalization_val = normalize_matrix(train=train, \n",
    "                                                              val=validation)\n",
    "    \n",
    "    X_test = df[df.semestr == test_sem]\n",
    "    \n",
    "    items = 1748\n",
    "    \n",
    "    (scores, hyperparams, model) = max(random_search(\n",
    "        normalization_train,\n",
    "        train, \n",
    "        validation,\n",
    "        items,\n",
    "        num_samples=10),\n",
    "        key=lambda x: x[0])\n",
    "    \n",
    "    dct = {}\n",
    "    dct['semestr'] = test_sem\n",
    "    print(f'\\n\\t\\tLightFM model Metrics for {test_sem} semester\\n') \n",
    "\n",
    "    for n in [1, 5, 10]:\n",
    "        hr = 100 * round(hit_rate_lightfm(model, train, X_test, items, test_sem, n), 3)\n",
    "        dct[f'HitRate@{n}'] = hr\n",
    "        print(f'\\tHit Rate@{n}:\\t\\t{hr:.2f}%')\n",
    "\n",
    "    print('\\n\\n')\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO63jN-VVXIw"
   },
   "source": [
    "## Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "VwW4zrw-VXIw"
   },
   "outputs": [],
   "source": [
    "categories = add_categories(df)\n",
    "\n",
    "df = pd.merge(df, categories.drop_duplicates(), left_on=['id', 'id_book'], right_on=['id', 'id_book'], how='inner').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-v-ZUb7VXIw",
    "outputId": "1d0bc071-b8f7-4720-96bf-1012e101cdd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semester: 2\n",
      "\n",
      "\t\tALS model Metrics for 2 semester\n",
      "\n",
      "\tHit Rate@1:\t\t6.30%\n",
      "\tHit Rate@5:\t\t23.20%\n",
      "\tHit Rate@10:\t\t28.70%\n",
      "\n",
      "\n",
      "\n",
      "\t\tCluster model Metrics for 2 semester with 7 clusters\n",
      "\tHit Rate@1:\t\t10.0%\n",
      "\tHit Rate@5:\t\t37.0%\n",
      "\tHit Rate@10:\t\t55.00000000000001%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tLightFM model Metrics for 2 semester\n",
      "\n",
      "\tHit Rate@1:\t\t2.90%\n",
      "\tHit Rate@5:\t\t13.90%\n",
      "\tHit Rate@10:\t\t24.40%\n",
      "\n",
      "\n",
      "\n",
      "semester: 3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lst = [1]\n",
    "report_cluster = []\n",
    "report_als = []\n",
    "report_lfm = []\n",
    "\n",
    "for n in range(2, 13):\n",
    "    print(f'semester: {n}')\n",
    "    dct2 = test_als(df, lst, n)\n",
    "    report_als.append(dct2)\n",
    "    \n",
    "    dct1 = test_clustering(df, lst, n)\n",
    "    report_cluster.append(dct1)\n",
    "\n",
    "    dct3 = test_lightfm(df, lst, n)\n",
    "    report_lfm.append(dct3)\n",
    "    lst.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuqyCpR6RIbY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "J9TpP3u0VXIv",
    "7FLAZ3MYe9u_"
   ],
   "name": "kmean+als+lightfm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
